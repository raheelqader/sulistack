{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import time\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=1986\n",
    "kfold_n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = pd.read_csv('../input/train_X.csv').fillna('')\n",
    "train_y = pd.read_csv('../input/train_y.csv', header=None).values\n",
    "train_y = train_y.reshape((train_y.shape[0],))\n",
    "\n",
    "\n",
    "test_X = pd.read_csv('../input/test_X.csv').fillna('')\n",
    "test_id = pd.read_csv('../input/test_id.csv', header=None).values\n",
    "test_id = test_id.reshape((test_id.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_rows = range(train_X.shape[0])\n",
    "random.shuffle(random_rows)\n",
    "\n",
    "random_rows = random.sample(random_rows, 10000)\n",
    "\n",
    "train_X = train_X.iloc[random_rows]\n",
    "train_y = train_y[random_rows]\n",
    "test_X = test_X.iloc[random_rows]\n",
    "\n",
    "train_X = train_X.replace([np.inf, -np.inf], 0)\n",
    "test_X = test_X.replace([np.inf, -np.inf], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<<<<<<<<< layer1 >>>>>>>>>>>\n",
      "fold 1/5\n",
      "1/10 fitting lgb on cv data\n",
      "completed in 0.0942349433899 seconds\n",
      "('log_loss: ', 0.52009199109340543)\n",
      "2/10 fitting lgb on cv data\n",
      "completed in 0.0363919734955 seconds\n",
      "('log_loss: ', 0.51916085950385071)\n",
      "3/10 fitting xgboost on cv data\n",
      "completed in 0.128356218338 seconds\n",
      "('log_loss: ', 0.44231312339571582)\n",
      "4/10 fitting xgboost on cv data\n",
      "completed in 0.0672469139099 seconds\n",
      "('log_loss: ', 0.44415445199821785)\n",
      "5/10 fitting svm on cv data\n",
      "completed in 35.7327029705 seconds\n",
      "('log_loss: ', 0.46571196136652604)\n",
      "6/10 fitting svm on cv data\n",
      "completed in 20.6488759518 seconds\n",
      "('log_loss: ', 0.45514942704381539)\n",
      "7/10 fitting LogisticRegression on cv data\n",
      "completed in 0.671350955963 seconds\n",
      "('log_loss: ', 0.38045537721687273)\n",
      "8/10 fitting LogisticRegression on cv data\n",
      "completed in 0.131936073303 seconds\n",
      "('log_loss: ', 0.39221646117291276)\n",
      "9/10 fitting RandomForestClassifier on cv data\n",
      "completed in 1.48125696182 seconds\n",
      "('log_loss: ', 0.34435278873020403)\n",
      "10/10 fitting RandomForestClassifier on cv data\n",
      "completed in 1.14947080612 seconds\n",
      "('log_loss: ', 0.34318676985030228)\n",
      "fold 2/5\n",
      "1/10 fitting lgb on cv data\n",
      "completed in 0.0607578754425 seconds\n",
      "('log_loss: ', 0.51630342302762444)\n",
      "2/10 fitting lgb on cv data\n",
      "completed in 0.0290439128876 seconds\n",
      "('log_loss: ', 0.51863414062954238)\n",
      "3/10 fitting xgboost on cv data\n",
      "completed in 0.111522912979 seconds\n",
      "('log_loss: ', 0.4388920111634742)\n",
      "4/10 fitting xgboost on cv data\n",
      "completed in 0.0631580352783 seconds\n",
      "('log_loss: ', 0.44574655104672412)\n",
      "5/10 fitting svm on cv data\n",
      "completed in 36.0970849991 seconds\n",
      "('log_loss: ', 0.47432904401052489)\n",
      "6/10 fitting svm on cv data\n",
      "completed in 20.2302680016 seconds\n",
      "('log_loss: ', 0.45604505936583883)\n",
      "7/10 fitting LogisticRegression on cv data\n",
      "completed in 0.612940073013 seconds\n",
      "('log_loss: ', 0.38586233943193987)\n",
      "8/10 fitting LogisticRegression on cv data\n",
      "completed in 0.137518882751 seconds\n",
      "('log_loss: ', 0.39981029804902207)\n",
      "9/10 fitting RandomForestClassifier on cv data\n",
      "completed in 1.37997889519 seconds\n",
      "('log_loss: ', 0.36383444143684518)\n",
      "10/10 fitting RandomForestClassifier on cv data\n",
      "completed in 1.16295695305 seconds\n",
      "('log_loss: ', 0.36194125758346324)\n",
      "fold 3/5\n",
      "1/10 fitting lgb on cv data\n",
      "completed in 0.0794050693512 seconds\n",
      "('log_loss: ', 0.51438953218785388)\n",
      "2/10 fitting lgb on cv data\n",
      "completed in 0.0474030971527 seconds\n",
      "('log_loss: ', 0.51853190669129179)\n",
      "3/10 fitting xgboost on cv data\n",
      "completed in 0.167958021164 seconds\n",
      "('log_loss: ', 0.43620410102605822)\n",
      "4/10 fitting xgboost on cv data\n",
      "completed in 0.076376914978 seconds\n",
      "('log_loss: ', 0.43772031328082084)\n",
      "5/10 fitting svm on cv data\n",
      "completed in 38.0688090324 seconds\n",
      "('log_loss: ', 0.46618883916073162)\n",
      "6/10 fitting svm on cv data\n",
      "completed in 23.3864300251 seconds\n",
      "('log_loss: ', 0.46283400306926015)\n",
      "7/10 fitting LogisticRegression on cv data\n",
      "completed in 1.39563202858 seconds\n",
      "('log_loss: ', 0.38172660271470898)\n",
      "8/10 fitting LogisticRegression on cv data\n",
      "completed in 0.269098043442 seconds\n",
      "('log_loss: ', 0.39321875483567514)\n",
      "9/10 fitting RandomForestClassifier on cv data\n",
      "completed in 2.10568594933 seconds\n",
      "('log_loss: ', 0.34426608747694792)\n",
      "10/10 fitting RandomForestClassifier on cv data\n",
      "completed in 1.32606601715 seconds\n",
      "('log_loss: ', 0.34383982898619458)\n",
      "fold 4/5\n",
      "1/10 fitting lgb on cv data\n",
      "completed in 0.0597438812256 seconds\n",
      "('log_loss: ', 0.51293793298672441)\n",
      "2/10 fitting lgb on cv data\n",
      "completed in 0.0329110622406 seconds\n",
      "('log_loss: ', 0.51561975054554343)\n",
      "3/10 fitting xgboost on cv data\n",
      "completed in 0.11356306076 seconds\n",
      "('log_loss: ', 0.434672658050758)\n",
      "4/10 fitting xgboost on cv data\n",
      "completed in 0.0564448833466 seconds\n",
      "('log_loss: ', 0.43451466621190921)\n",
      "5/10 fitting svm on cv data\n",
      "completed in 36.0487210751 seconds\n",
      "('log_loss: ', 0.46849298007904716)\n",
      "6/10 fitting svm on cv data\n",
      "completed in 21.5303490162 seconds\n",
      "('log_loss: ', 0.4464424141274187)\n",
      "7/10 fitting LogisticRegression on cv data\n",
      "completed in 0.649928092957 seconds\n",
      "('log_loss: ', 0.37234764105886675)\n",
      "8/10 fitting LogisticRegression on cv data\n",
      "completed in 0.165233135223 seconds\n",
      "('log_loss: ', 0.38695324703928291)\n",
      "9/10 fitting RandomForestClassifier on cv data\n",
      "completed in 1.50875377655 seconds\n",
      "('log_loss: ', 0.33292136829664865)\n",
      "10/10 fitting RandomForestClassifier on cv data\n",
      "completed in 1.17389512062 seconds\n",
      "('log_loss: ', 0.32973720008010859)\n",
      "fold 5/5\n",
      "1/10 fitting lgb on cv data\n",
      "completed in 0.0621440410614 seconds\n",
      "('log_loss: ', 0.51760664601595596)\n",
      "2/10 fitting lgb on cv data\n",
      "completed in 0.0307970046997 seconds\n",
      "('log_loss: ', 0.52062887295016858)\n",
      "3/10 fitting xgboost on cv data\n",
      "completed in 0.109310865402 seconds\n",
      "('log_loss: ', 0.44223850363937245)\n",
      "4/10 fitting xgboost on cv data\n",
      "completed in 0.056077003479 seconds\n",
      "('log_loss: ', 0.44198939782670282)\n",
      "5/10 fitting svm on cv data\n",
      "completed in 42.6880819798 seconds\n",
      "('log_loss: ', 0.47159456111567638)\n",
      "6/10 fitting svm on cv data\n",
      "completed in 23.165459156 seconds\n",
      "('log_loss: ', 0.45841060275397072)\n",
      "7/10 fitting LogisticRegression on cv data\n",
      "completed in 2.11450099945 seconds\n",
      "('log_loss: ', 0.38017010208972429)\n",
      "8/10 fitting LogisticRegression on cv data\n",
      "completed in 0.304184913635 seconds\n",
      "('log_loss: ', 0.39332486454562365)\n",
      "9/10 fitting RandomForestClassifier on cv data\n",
      "completed in 1.51741600037 seconds\n",
      "('log_loss: ', 0.3454695919154433)\n",
      "10/10 fitting RandomForestClassifier on cv data\n",
      "completed in 1.16469097137 seconds\n",
      "('log_loss: ', 0.35908675335159629)\n",
      "1/10 fitting lgb on full train data\n",
      "completed in 0.0720348358154 seconds\n",
      "2/10 fitting lgb on full train data\n",
      "completed in 0.0358850955963 seconds\n",
      "3/10 fitting xgboost on full train data\n",
      "completed in 0.15275812149 seconds\n",
      "4/10 fitting xgboost on full train data\n",
      "completed in 0.074227809906 seconds\n",
      "5/10 fitting svm on full train data\n",
      "completed in 84.0448679924 seconds\n",
      "6/10 fitting svm on full train data\n",
      "completed in 39.2168929577 seconds\n",
      "7/10 fitting LogisticRegression on full train data\n",
      "completed in 1.98413896561 seconds\n",
      "8/10 fitting LogisticRegression on full train data\n",
      "completed in 0.32604598999 seconds\n",
      "9/10 fitting RandomForestClassifier on full train data\n",
      "completed in 1.91996502876 seconds\n",
      "10/10 fitting RandomForestClassifier on full train data\n",
      "completed in 1.52236604691 seconds\n",
      "<<<<<<<<<< layer2 >>>>>>>>>>>\n",
      "fold 1/5\n",
      "1/2 fitting xgboost on cv data\n",
      "completed in 0.146403074265 seconds\n",
      "('log_loss: ', 0.42883045189473584)\n",
      "2/2 fitting LogisticRegression on cv data\n",
      "completed in 1.93731379509 seconds\n",
      "('log_loss: ', 0.34062510833355564)\n",
      "fold 2/5\n",
      "1/2 fitting xgboost on cv data\n",
      "completed in 0.201781988144 seconds\n",
      "('log_loss: ', 0.4296570436424878)\n",
      "2/2 fitting LogisticRegression on cv data\n",
      "completed in 1.39846611023 seconds\n",
      "('log_loss: ', 0.34874341783445717)\n",
      "fold 3/5\n",
      "1/2 fitting xgboost on cv data\n",
      "completed in 0.199881076813 seconds\n",
      "('log_loss: ', 0.429658107444644)\n",
      "2/2 fitting LogisticRegression on cv data\n",
      "completed in 1.18577384949 seconds\n",
      "('log_loss: ', 0.33934914709901542)\n",
      "fold 4/5\n",
      "1/2 fitting xgboost on cv data\n",
      "completed in 0.19021987915 seconds\n",
      "('log_loss: ', 0.4209137378319196)\n",
      "2/2 fitting LogisticRegression on cv data\n",
      "completed in 1.28266191483 seconds\n",
      "('log_loss: ', 0.32600109089667184)\n",
      "fold 5/5\n",
      "1/2 fitting xgboost on cv data\n",
      "completed in 0.173763036728 seconds\n",
      "('log_loss: ', 0.43185678228042673)\n",
      "2/2 fitting LogisticRegression on cv data\n",
      "completed in 1.31695103645 seconds\n",
      "('log_loss: ', 0.34220073467777168)\n",
      "1/2 fitting xgboost on full train data\n",
      "completed in 0.231234073639 seconds\n",
      "2/2 fitting LogisticRegression on full train data\n",
      "completed in 1.74257898331 seconds\n",
      "<<<<<<<<<< layer3 >>>>>>>>>>>\n",
      "fold 1/5\n",
      "1/1 fitting xgboost on cv data\n",
      "completed in 0.0907030105591 seconds\n",
      "('log_loss: ', 0.43060607004022672)\n",
      "fold 2/5\n",
      "1/1 fitting xgboost on cv data\n",
      "completed in 0.0204498767853 seconds\n",
      "('log_loss: ', 0.42675815512989834)\n",
      "fold 3/5\n",
      "1/1 fitting xgboost on cv data\n",
      "completed in 0.0203440189362 seconds\n",
      "('log_loss: ', 0.42851765640079975)\n",
      "fold 4/5\n",
      "1/1 fitting xgboost on cv data\n",
      "completed in 0.0247759819031 seconds\n",
      "('log_loss: ', 0.42087892877155092)\n",
      "fold 5/5\n",
      "1/1 fitting xgboost on cv data\n",
      "completed in 0.022472858429 seconds\n",
      "('log_loss: ', 0.42787496115697388)\n",
      "1/1 fitting xgboost on full train data\n",
      "completed in 0.0270218849182 seconds\n",
      "('log_loss on train: ', 0.42692801527493573)\n"
     ]
    }
   ],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self, model_type, features=[]):\n",
    "        self.model = None\n",
    "        self.model_type = model_type\n",
    "        self.features = features\n",
    "\n",
    "    # initialize and fit xgboost model\n",
    "    def xgb_model(self, train_X, train_y, seed_val=seed, num_rounds=10):\n",
    "\n",
    "        param = {}\n",
    "        param['objective'] = 'binary:logistic'\n",
    "        param['eval_metric'] = 'logloss'\n",
    "        param['eta'] = 0.1\n",
    "        param['max_depth'] = 6\n",
    "        param['silent'] = 1\n",
    "        param['subsample'] = 0.8\n",
    "        param['colsample_bytree'] = 0.8\n",
    "        param['min_child_weight'] = 8\n",
    "\n",
    "\n",
    "        param['nthread'] = 3\n",
    "        param['seed'] = seed_val\n",
    "        num_rounds = num_rounds\n",
    "\n",
    "        plst = list(param.items())\n",
    "        xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, verbose_eval=True)\n",
    "\n",
    "\n",
    "        return model\n",
    "\n",
    "    \n",
    "    # initialize and fit lightgbm model\n",
    "    def lgb_model(self, train_X, train_y, seed_val=seed, num_rounds=10):\n",
    "\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'metric': 'binary_logloss',\n",
    "            'num_leaves': 31,\n",
    "            'learning_rate': 0.05,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': 0\n",
    "        }\n",
    "        \n",
    "        lgb_train = lgb.Dataset(train_X, train_y)\n",
    "        model = lgb.train(params, lgb_train, num_boost_round=num_rounds)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    # get predictions \n",
    "    def predict(self, test_X, test_y=None):\n",
    "\n",
    "        if self.features:\n",
    "            test_X = test_X[self.features]\n",
    "            \n",
    "        if self.model:\n",
    "            \n",
    "            if self.model_type == 'xgboost':\n",
    "                xgtest = xgb.DMatrix(test_X)\n",
    "                preds = self.model.predict(xgtest)\n",
    "                preds = preds.reshape(-1, 1)\n",
    "            \n",
    "            elif self.model_type == 'lgb':\n",
    "                preds = self.model.predict(test_X)\n",
    "                preds = preds.reshape(-1, 1)\n",
    "\n",
    "            else:\n",
    "                preds = self.model.predict_proba(test_X)[:,1]\n",
    "                preds = preds.reshape(-1, 1)\n",
    "                \n",
    "            if test_y is not None:\n",
    "                print('log_loss: ', log_loss(test_y, preds))\n",
    "\n",
    "            return preds\n",
    "        else:\n",
    "            assert('No trained model was found... You have to first fit the model')\n",
    "\n",
    "    # fit model on full feature set or subset if provided \n",
    "    def fit(self, train_X, train_y):\n",
    "\n",
    "        if self.features:\n",
    "            train_X = train_X[self.features]\n",
    "\n",
    "            \n",
    "        if self.model_type == 'xgboost':\n",
    "            self.model = self.xgb_model(train_X, train_y)\n",
    "        \n",
    "        if self.model_type == 'lgb':\n",
    "            self.model = self.lgb_model(train_X, train_y)\n",
    "            \n",
    "            \n",
    "        elif self.model_type == 'RandomForestClassifier':\n",
    "            self.model = RandomForestClassifier(n_estimators=150, n_jobs=-1)\n",
    "            self.model.fit(train_X, train_y)\n",
    "            \n",
    "        elif self.model_type == 'LogisticRegression':\n",
    "            self.model = LogisticRegression()\n",
    "            self.model.fit(train_X, train_y)\n",
    "            \n",
    "        elif self.model_type == 'svm':\n",
    "            self.model = SVC(random_state=seed, probability=True, verbose=True)\n",
    "            self.model.fit(train_X, train_y)\n",
    "\n",
    "            \n",
    "\n",
    "class Layer:\n",
    "    \n",
    "    def __init__(self, models=[], injected_features=[]):\n",
    "            \n",
    "        if models:\n",
    "            self.models = models\n",
    "        else:\n",
    "            self.models=[]\n",
    "        \n",
    "        if injected_features:\n",
    "            self.injected_features = injected_features\n",
    "        else:\n",
    "            self.injected_features = []\n",
    "        \n",
    "    \n",
    "    \n",
    "    def add_model(self, model):\n",
    "        self.models.append(model)\n",
    "\n",
    "        \n",
    "\n",
    "    def fit_layer(self, train_X, train_y, test_X):\n",
    "        \n",
    "        pred_trainX={}\n",
    "        pred_testX={}\n",
    "        \n",
    "        for m_i, model in enumerate(self.models):\n",
    "            pred_trainX[m_i] = np.zeros((train_X.shape[0], 1))\n",
    "\n",
    "        \n",
    "        kfold = model_selection.StratifiedKFold(n_splits=kfold_n, shuffle=True, random_state=seed)\n",
    "        \n",
    "        for fold, (train_index, val_index) in enumerate(kfold.split(train_X, train_y)):\n",
    "            print('fold {}/{}'.format(fold+1, kfold_n))\n",
    "            _train_y, _val_y = train_y[train_index], train_y[val_index]\n",
    "            _train_X, _val_X = train_X.iloc[train_index], train_X.iloc[val_index]\n",
    "            \n",
    "            # fit model on cv train data and get predictions predict for cv test data\n",
    "            for m_i, model in enumerate(self.models):\n",
    "\n",
    "                print('{}/{} fitting {} on cv data'.format(m_i+1, len(self.models), model.model_type))\n",
    "                start_time = time.time()\n",
    "                model.fit(_train_X, _train_y)\n",
    "                print('completed in {} seconds'.format(time.time() - start_time))\n",
    "                \n",
    "                preds = model.predict(_val_X, _val_y)\n",
    "                pred_trainX[m_i][val_index, :] = preds\n",
    "                \n",
    "        # fit model on full train data and get predictions for full test data\n",
    "        for m_i, model in enumerate(self.models):\n",
    "            \n",
    "            print('{}/{} fitting {} on full train data'.format(m_i+1, len(self.models), model.model_type))\n",
    "            start_time = time.time()\n",
    "            model.fit(train_X, train_y)\n",
    "            print('completed in {} seconds'.format(time.time() - start_time))\n",
    "            \n",
    "            preds = model.predict(test_X)\n",
    "            pred_testX[m_i] = preds\n",
    "            \n",
    "        # store predictions in data frames  \n",
    "        pred_trainX_df = pd.DataFrame()\n",
    "        pred_testX_df = pd.DataFrame()\n",
    "\n",
    "        for m_i, model in enumerate(self.models):\n",
    "            pred_trainX_df[m_i] = pred_trainX[m_i].flatten()\n",
    "            pred_testX_df[m_i] = pred_testX[m_i].flatten()\n",
    "        \n",
    "        \n",
    "        return pred_trainX_df, pred_testX_df\n",
    "\n",
    "        \n",
    "class SuliStack():\n",
    "    \n",
    "    \n",
    "    def __init__(self, layers=[]):\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        \n",
    "    def fit(self, train_X, train_y, test_X):\n",
    "                \n",
    "        pred_trainX = pd.DataFrame()\n",
    "        pred_testX = pd.DataFrame()\n",
    "\n",
    "        for l_i, layer in enumerate(self.layers):\n",
    "            print('<<<<<<<<<< layer{} >>>>>>>>>>>'.format(l_i+1))\n",
    "  \n",
    "            \n",
    "            if pred_trainX.empty and pred_testX.empty:\n",
    "                pred_trainX, pred_testX = layer.fit_layer(train_X, train_y, test_X)\n",
    "            else:\n",
    "                \n",
    "                if layer.injected_features:\n",
    "                    \n",
    "                    for feature in layer.injected_features:\n",
    "                        pred_trainX[feature] = pd.Series(train_X[feature].values)\n",
    "                        pred_testX[feature] = pd.Series(test_X[feature].values)\n",
    "                        \n",
    "                pred_trainX, pred_testX = layer.fit_layer(pred_trainX, train_y, pred_testX)\n",
    "            \n",
    "            del layer #delete#####################################\n",
    "        \n",
    "        return pred_trainX, pred_testX\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "ss = SuliStack()\n",
    "\n",
    "#initliaze layer1 models\n",
    "clf_l1_lgb = Model('lgb')\n",
    "clf_l1_lgb_best_feat = Model('lgb', ['z_word_match', 'wup_similarity', 'z_tfidf_len1', 'z_tfidf_len2', 'norm_wmd', 'z_tfidf_sum2', 'skew_q2vec', 'euclidean_distance', 'kur_q1vec', 'fuzz_WRatio', 'fuzz_partial_ratio', 'len_word_q1', 'skew_q1vec', 'len_q1', 'wmd', 'len_char_q2', 'str_levenshtein_2', 'cityblock_distance', 'len_word_q2', 'common_bigram_ratio'])\n",
    "clf_l1_xgboost = Model('xgboost')\n",
    "clf_l1_xgboost_best_feat = Model('xgboost', ['z_word_match', 'wup_similarity', 'z_tfidf_len1', 'z_tfidf_len2', 'norm_wmd', 'z_tfidf_sum2', 'skew_q2vec', 'euclidean_distance', 'kur_q1vec', 'fuzz_WRatio', 'fuzz_partial_ratio', 'len_word_q1', 'skew_q1vec', 'len_q1', 'wmd', 'len_char_q2', 'str_levenshtein_2', 'cityblock_distance', 'len_word_q2', 'common_bigram_ratio'])\n",
    "clf_l1_svm = Model('svm')\n",
    "clf_l1_svm_best_feat = Model('svm', ['z_word_match', 'wup_similarity', 'z_tfidf_len1', 'z_tfidf_len2', 'norm_wmd', 'z_tfidf_sum2', 'skew_q2vec', 'euclidean_distance', 'kur_q1vec', 'fuzz_WRatio', 'fuzz_partial_ratio', 'len_word_q1', 'skew_q1vec', 'len_q1', 'wmd', 'len_char_q2', 'str_levenshtein_2', 'cityblock_distance', 'len_word_q2', 'common_bigram_ratio'])\n",
    "clf_l1_lr = Model('LogisticRegression')\n",
    "clf_l1_lr_best_feat = Model('LogisticRegression', ['z_word_match', 'wup_similarity', 'z_tfidf_len1', 'z_tfidf_len2', 'norm_wmd', 'z_tfidf_sum2', 'skew_q2vec', 'euclidean_distance', 'kur_q1vec', 'fuzz_WRatio', 'fuzz_partial_ratio', 'len_word_q1', 'skew_q1vec', 'len_q1', 'wmd', 'len_char_q2', 'str_levenshtein_2', 'cityblock_distance', 'len_word_q2', 'common_bigram_ratio'])\n",
    "clf_l1_rf = Model('RandomForestClassifier')\n",
    "clf_l1_rf_best_feat = Model('RandomForestClassifier', ['z_word_match', 'wup_similarity', 'z_tfidf_len1', 'z_tfidf_len2', 'norm_wmd', 'z_tfidf_sum2', 'skew_q2vec', 'euclidean_distance', 'kur_q1vec', 'fuzz_WRatio', 'fuzz_partial_ratio', 'len_word_q1', 'skew_q1vec', 'len_q1', 'wmd', 'len_char_q2', 'str_levenshtein_2', 'cityblock_distance', 'len_word_q2', 'common_bigram_ratio'])\n",
    "\n",
    "#initliaze layer2 models\n",
    "clf_l2_xgboost = Model('xgboost')\n",
    "clf_l2_lr = Model('LogisticRegression')\n",
    "\n",
    "#initliaze layer2 models\n",
    "clf_l3_xgboost = Model('xgboost')\n",
    "\n",
    "\n",
    "#add models to layer1\n",
    "layer_1 = Layer()\n",
    "layer_1.add_model(clf_l1_lgb)\n",
    "layer_1.add_model(clf_l1_lgb_best_feat)\n",
    "layer_1.add_model(clf_l1_xgboost)\n",
    "layer_1.add_model(clf_l1_xgboost_best_feat)\n",
    "layer_1.add_model(clf_l1_svm)\n",
    "layer_1.add_model(clf_l1_svm_best_feat)\n",
    "layer_1.add_model(clf_l1_lr)\n",
    "layer_1.add_model(clf_l1_lr_best_feat)\n",
    "layer_1.add_model(clf_l1_rf)\n",
    "layer_1.add_model(clf_l1_rf_best_feat)\n",
    "\n",
    "#add models to layer2\n",
    "layer_2 = Layer(injected_features=list(train_X.columns.values))\n",
    "layer_2.add_model(clf_l2_xgboost)\n",
    "layer_2.add_model(clf_l2_lr)\n",
    "\n",
    "#add models to layer3\n",
    "layer_3 = Layer()\n",
    "layer_3.add_model(clf_l3_xgboost)\n",
    "\n",
    "\n",
    "#add layers to stack\n",
    "ss.add_layer(layer_1)\n",
    "ss.add_layer(layer_2)\n",
    "ss.add_layer(layer_3)\n",
    "\n",
    "\n",
    "pred_trainX, pred_testX = ss.fit(train_X, train_y, test_X)\n",
    "print('log_loss on train: ', log_loss(train_y, pred_trainX))\n",
    "\n",
    "out_df = pd.DataFrame()\n",
    "out_df['test_id'] = test_id\n",
    "out_df['is_duplicate'] = pred_testX\n",
    "out_df.to_csv('simple_xgb.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
